[{"authors":["nico"],"categories":null,"content":"I\u0026rsquo;m currently an associate scientist in the Intelligent Unmanned System Group, Temasek Laboratories. Previously, I was working as a research engineer in the Biorobotics Lab, National University of Singapore under Dr. Haoyong Yu’s supervision. I was also a member of the State Key Laboratory of Robotics which is part of the Chinese Academy of Sciences.\nMy research interests includes 3D visual perception and navigation, data science, machine learning, and autonomous systems. Please feel free to contact me for any cooperation.\n","date":1593302400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1593302400,"objectID":"eceb09896bb9761ffd0d3445c2c97068","permalink":"/author/yu-zhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yu-zhou/","section":"authors","summary":"I\u0026rsquo;m currently an associate scientist in the Intelligent Unmanned System Group, Temasek Laboratories. Previously, I was working as a research engineer in the Biorobotics Lab, National University of Singapore under Dr.","tags":null,"title":"Yu Zhou","type":"authors"},{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/nelson-bighetti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nelson-bighetti/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Yu Zhou","Shupeng Lai","Huimin Cheng","Zhi Gao","Feng lin","Ben M. Chen"],"categories":null,"content":"","date":1593302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593302400,"objectID":"3ce27cf265544b5b32d39d90822612e4","permalink":"/publication/toward-autonomy-of-micro-aerial-vehicles-in-unknown-and-global-positioning-system-denied-environments/","publishdate":"2020-06-28T00:00:00Z","relpermalink":"/publication/toward-autonomy-of-micro-aerial-vehicles-in-unknown-and-global-positioning-system-denied-environments/","section":"publication","summary":"In this paper, we present a comprehensive design and implementation for a micro aerial vehicle (MAV) that is able to perform 3D autonomous navigation and obstacle avoidance in cluttered and realistic unknown environments without the aid of GPS and other external sensors or markers. To achieve these autonomous missions, modularized components are developed for the MAV, including visual inertial odometry (VIO), 3D occupancy mapping and motion planning. The proposed system is implemented to run on a small embedded computer in real-time. It is demonstrated to be robust in both simulation and real flight experiments.","tags":null,"title":"Toward Autonomy of Micro Aerial Vehicles in Unknown and Global Positioning System Denied Environments","type":"publication"},{"authors":[],"categories":null,"content":"-- -- I recently gave a tutorial talk on the front-end of visual-inertial odometry. Check it out by clicking the pdf label on top of the page!\n","date":1592485200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592485200,"objectID":"54171852986181c01af933afc1b9c4b0","permalink":"/post/vio-talk/","publishdate":"2020-06-18T13:00:00Z","relpermalink":"/post/vio-talk/","section":"post","summary":"A tutorial talk on the front-end of visual-inertial odometry","tags":[],"title":"The Front End of SLAM","type":"post"},{"authors":["Pengfei Wang","Sunan Huang","Yang Hong","Sutthiphong Srigrarom","William Wai Lun Leong","Zhengtian Ma","Yu Zhou","Jingxuan Sun","Mohamed Redhwan Bin Abdul","Rodney Swee Huat Teo"],"categories":null,"content":"","date":1591056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591056000,"objectID":"1bd3647c8ef4b7cc66b4a813ec524cef","permalink":"/publication/low-cost-camera-based-sense-and-avoid-in-unmanned-aerial-vehicles-sensing-and-control-methods/","publishdate":"2020-06-02T00:00:00Z","relpermalink":"/publication/low-cost-camera-based-sense-and-avoid-in-unmanned-aerial-vehicles-sensing-and-control-methods/","section":"publication","summary":"In this paper, a low-cost camera based sense and avoid control scheme is proposed. First, for the unmanned aerial vehicle (UAV) sense and avoid problem, we propose and demonstrate a low cost solution with only one monocular, wide field of view camera. We adapted a deep learning based algorithm and made it lightweight enough to run on-board the UAV. We created a synthetic dataset to increase the size of the traning data set. Second, we developed a filter to estimate the 3D position and velocity of the moving obstacle. Third, based on the estimated information, we adapted the velocity obstacle approach to work in 3D. Finally, we implemented the algorithms in the software framework of our UAV testbed and conducted flight tests, demonstrating the effectiveness of the solution.","tags":null,"title":"Low-cost camera based sense and avoid in unmanned aerial vehicles: sensing and control methods","type":"publication"},{"authors":null,"categories":null,"content":"From January 2019, I worked on the “Autonomy of Micro Aerial Vehicles in Unknown Environments” project. The objective is to make MAV system that can navigate autonomously in GPS-denied and obstacle-cluttered environments while avoiding obstacles on the way. Various advanced technologies have been designed, including a stereo visual-inertial state estimation that can handle scenarios without external localization resource, a GPU-based EDF mapping that significantly improves the real-time performance without sacrificing accuracy as well as a model predictive local motion planning with BSCPs solved by PSO, providing increased performance for tasks that require precise and timely maneuver. The results of simulation and real flight testing in both indoor and semi-open scenarios with various tasks have proved the effectiveness of the proposed system for challenging practical applications.\nThis work has resulted in the paper \u0026ldquo;Toward Autonomy of Micro Aerial Vehicles in Unknown and Global Positioning System Denied Environments\u0026rdquo; published in the IEEE Transactions on Industrial Electronics.\n  Demo ","date":1590940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590940800,"objectID":"914e01a89208ae6093924b1d23401180","permalink":"/project/toward-autonomy-of-micro-aerial-vehicles-in-unknown-environments/","publishdate":"2020-06-01T00:00:00+08:00","relpermalink":"/project/toward-autonomy-of-micro-aerial-vehicles-in-unknown-environments/","section":"project","summary":"We present a comprehensive design and implementation for a micro aerial vehicle (MAV) that is able to perform 3D autonomous navigation and obstacle avoidance in cluttered and realistic unknown environments without the aid of GPS and other external sensors or markers.","tags":["UAV","Robotics","Autonomous Robot","Motion Planning","Mapping","GPU","VIO","ROS"],"title":"Toward Autonomy of Micro Aerial Vehicles in Unknown Environments","type":"project"},{"authors":null,"categories":null,"content":"overview Ｔhis is for web-based UI shown as the image. It can connect to multiple drones by their IP address and show status of each drone, such as positon, image, healthy status etc. User can sent commands (takeoff, mission, swarm, hover, landing) to drone directly from the UI. Moreover, a Google map is inserted to show realtime location and trajectory of the drones.\nTutorials and useful websites   Bootstrap 4 + ROS  Bootstrap 4 Tutorial  ROBOTWEBTOOLS  Theme  HTML color code  ","date":1564588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564588800,"objectID":"e894202a98557059e4795fe07f7bbc94","permalink":"/post/robot_web_ui/","publishdate":"2019-08-01T00:00:00+08:00","relpermalink":"/post/robot_web_ui/","section":"post","summary":"A web-based multi UAV ground station software designed for UAV swarm intelligence","tags":["UAV","web-based UI"],"title":"Robot web-based user interface","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Yu Zhou","Geng Qin","Feng Lin"],"categories":null,"content":"","date":1540215854,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540215854,"objectID":"f62f2f0b6bd4cb6b2fe3230d69087081","permalink":"/publication/development-of-nano-uav-platform-for-navigation-in-gps-denied-environment-using-snapdragon/","publishdate":"2018-10-22T21:44:14+08:00","relpermalink":"/publication/development-of-nano-uav-platform-for-navigation-in-gps-denied-environment-using-snapdragon/","section":"publication","summary":"In order to develop a nano UAV in a GPS-denied environment, a visual-inertial system (VINS) consisting of a monocular camera and a low-cost inertial measurement unit (IMU) should be integrated, as they make a minimum, ultra lightweight sensor suite that enables both autonomous flight and sufficient environment awareness. Although there are numerous potential applications, getting to understand the structure of the flight platform can be cumbersome and confusing. Packages and resources provided on the Qualcomm developer network (QCN) and their official GitHub group ATLFlight contain un-unified and disorderly chunks of information. In this paper, we present an organized documentation that presents in detail, the platform structure as a whole, from hardware to software architecture, including installation and API related applications.","tags":null,"title":"Development of Nano UAV Platform for Navigation in Gps Denied Environment Using Snapdragon","type":"publication"},{"authors":["Mingjie Lao","Xudong Chen","Feng Lin","Geng Qin","Wenqi Liu","Yu Zhou"],"categories":null,"content":"","date":1529675054,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529675054,"objectID":"ce0a2f784c2442d57f0512489b3a4532","permalink":"/publication/visual-target-detection-and-tracking-framework-using-deep-convolutional-neural-networks-for-micro-aerial-vehicles/","publishdate":"2018-06-22T21:44:14+08:00","relpermalink":"/publication/visual-target-detection-and-tracking-framework-using-deep-convolutional-neural-networks-for-micro-aerial-vehicles/","section":"publication","summary":"This paper presents a visual detection and tracking framework which estimates a smooth target position for various applications on micro aerial vehicles (MAVs). The proposed framework consists of two major components, a deep learning-based detector and a correlation ﬁlter-based tracker.The detector running at a low frequency ﬁrst detects a targetand initializes the tracker. The estimated target position from the tracker will be updated by the detector when the detectionconﬁdence is high or the tracking is considered fail. Due to the limited computational power on most MAV platforms, algorithms are implemented at two separated processing units.The detector runs at a ground control station (GCS) equipped with NVIDIA GTX 1060 while the tracker runs at a MAVonboard low-cost CPU. The transmission of image and target pose information is bridged via a high-speed Wi-Fi network to minimize the latency. In our experiment, the proposed framework is able to realize real-time detection and tracking with 30 frames per second (FPS) on our system.","tags":null,"title":"Visual Target Detection and Tracking Framework Using Deep Convolutional Neural Networks for Micro Aerial Vehicles","type":"publication"},{"authors":["Zhenxing Sun","Yu Zhou","Xinghua Zhang","Haoyong Yu"],"categories":null,"content":"","date":1527860654,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527860654,"objectID":"c8ece1a12621633514500ebbfdec9e04","permalink":"/publication/decentralized-robust-exact-tracking-control-for-2-dof-planar-robot-manipulator/","publishdate":"2018-06-01T21:44:14+08:00","relpermalink":"/publication/decentralized-robust-exact-tracking-control-for-2-dof-planar-robot-manipulator/","section":"publication","summary":"In this study, a novel decentralized exact tracking control approach for 2-DOF planar robot manipulators with disturbances and uncertainties has been proposed. The main idea of the proposed method is to partition the whole robot manipulator into single joint and lumped disturbance. A well designed extended high gain observer (EHGO) is adopted to observe the lumped disturbance of each joint, which includes coupling, load disturbance and uncertainty, etc. Rather than coping with the robot manipulator as a whole, the proposed method is locally designed to compensate the estimated coupling and stabilize the internal states of a single joint. By doing that, each joint can be virtually decoupled from the whole robot manipulator and functions in a relatively independent way. When joints are interconnected, the stabilization of the whole robot manipulator would be obtained by only guaranteeing the localized stability of individual joint. The theoretical analyses are rigorously conducted by utilizing Lyapunov stability theorem. Simulation results illustrate the effectiveness of the proposed control scheme.","tags":null,"title":"Decentralized robust exact tracking control for 2-DOF planar robot manipulator","type":"publication"},{"authors":null,"categories":null,"content":"From March to June 2018, I worked on the proof-of-concept phase of the “Dog-Drone” project.\nThe objective is to immobilize suspicious individuals in an indoor environment by flying multiple drones aggressively around them. My work includes the following:\n Human detection and tracking Trajectory generation System integration  This work has resulted in papers published in the IEEE ICCA and IECON 2018.\n  Dog-Drone demo At this stage, to prove the concept, we are using the off-the-shelf flight platform with a visual odometry integration. Right now we have built our drone platform CT-Line due to the limitation of the commercial product and have conducted tests of flying multiple drones aggressively in an indoor environment.\n","date":1525104000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525104000,"objectID":"e6a058fba656b6194a33bf929e9b66f4","permalink":"/project/dog-drone/","publishdate":"2018-05-01T00:00:00+08:00","relpermalink":"/project/dog-drone/","section":"project","summary":"To immobilize suspicious individuals in an indoor environment by flying multiple drones aggressively around them.","tags":["UAV","Robotics","Autonomous Robot","Computer Vision","Deep Learning","Localization","Human Tracking","ROS"],"title":"Dog-Drone","type":"project"},{"authors":null,"categories":null,"content":"From March 2017 to December 2018, I worked on “Project Micrathene” with a Ph.D. student who focused on controls. The objective of this project was to develop an autonomous launch, tracking and landing system for an Unmanned Aerial Vehicle (UAV). The object to track and land on was a moving ground platform, and everything was to be done without GPS.\nIn this project, vision is used to estimate the pose, the translation and rotation relationship, between the UGV and the UAV. The pose was used for flight control during different states of the UAV.\nThe hardware platform contains:\n DJI Matrice 100 fly platform Point Grey Chameleon 3 camera NUC onboard computer. Omnidirectional mobility platform  Communication between the onboard and offboard computer is established over Wi-Fi.\nThe software involves:\n Ubuntu ROS Kinetic OpenCV DJI SDK  Fig. 1. Hardware and Software Platform A marker was designed to obtain the relative pose between the UAV and the UGV platform. There are four coordinate systems: image, camera, marker and UAV. The transformation matrix between different coordinate systems is shown on Figure 2. n order to control the UAV, the pose between the camera and the marker, which is [R2 t2], has to be calculated with the PnP method. The pose between the UAV and the moving platform can then be obtained through another coordinate transform.\nFig. 2. Transformation matrix The software design of this project is shown on Figure 3:\n  Obtain undistorted images through image processing;\n  Send these images to the marker detection process to get the camera pose w.r.t. the marker;\n  In the main process, a state machine was run to process all the data and commands, including the pose between the moving platform and the UAV, sensor data from DJI SDK and the command from the off-board computer.\n  Fig. 3. Software design The final demonstration is shown in the following video:   Demo Video","date":1489507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489507200,"objectID":"28c2aaddac3aee58eddbc2eb08fefce2","permalink":"/project/gps-denied-vision-control-of-uav/","publishdate":"2017-03-15T00:00:00+08:00","relpermalink":"/project/gps-denied-vision-control-of-uav/","section":"project","summary":"Develop an autonomous launch, tracking and landing system for an Unmanned Aerial Vehicle (UAV).","tags":["UAV","Robotics","Autonomous Robot","Computer Vision","ROS","Matrice 100","DJI SDK"],"title":"GPS-Denied Vision Control of UAV","type":"project"},{"authors":null,"categories":null,"content":"This project was to calibrate the translation and rotation matrices between camera and laser coordinate systems to provide laser points with color information from the image for further applications such as segmentation.\nThe QR code board was used as a marker, which could be detected in both point clouds and image. I matched the center point pair of the QR code from the 2D points in the image and that of the 3D points in laser coordinates, and used the PnP method to get the relation between the two coordinates. The calibration procedure and fusion results are shown on Figure 1.\nFig. 1. Calibration Procedure   Video 1. Fusion results   Video 2. Fusion results","date":1483200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483200000,"objectID":"2b62ebeecf15bcd1515fe9801cba2003","permalink":"/post/laser-camera-calibration/","publishdate":"2017-01-01T00:00:00+08:00","relpermalink":"/post/laser-camera-calibration/","section":"post","summary":"This project was to calibrate the translation and rotation matrices between camera and laser coordinate systems to provide laser points with color information from the image for further applications such as segmentation.","tags":["Laser","Sensor Fusion","Computer Vision","ROS"],"title":"Laser Camera Fusion","type":"post"},{"authors":["Liying Yang","Bin Xiao","Yu Zhou","Yuqing He","Hongzhi Zhang","Jianda Han"],"categories":null,"content":"","date":1466603054,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466603054,"objectID":"633ca5421ae44969285e1e3544366c45","permalink":"/publication/a-robust-real-time-vision-based-gps-denied-navigation-system-of-uav/","publishdate":"2016-06-22T21:44:14+08:00","relpermalink":"/publication/a-robust-real-time-vision-based-gps-denied-navigation-system-of-uav/","section":"publication","summary":"In 2015 International UAV Innovation Grand Prix the competition, the cargo transport task is assumed as that there are 4 buckets placed in four circles on one moving platform. Firstly, the unmanned aerial vehicle (UAV) is required to identify circle targets and the black and white id marker near the circle on one moving platform, then the UAV chosen a target bucket, tracked and transported it to the other moving platform, until all 4 buckets are transported from one moving platform to the other. In order to accomplish the cargo transport task, a method of a real-time vision-based GPS-denied multiple object tracking for UAV is developed. The Pixhawk controller is used to achieve tracking, that the relative distance and velocity between the target and UAV is estimated by the image. Finally, the experimental results proved the effectiveness and robustness of the algorithm.","tags":null,"title":"A robust real-time vision based GPS-denied navigation system of UAV","type":"publication"},{"authors":null,"categories":null,"content":"The error-state Kalman filter (ESKF) is one of the tools we may use for combining IMU with magnetometer data to obtain a robust attitude estimation. It has many benefits such as avoiding issues related to over-parameterization and the consequent risk of the singularity of the involved covariance matrices. The formulation of the ESKF algorithm used for attitude estimation is as follows:\nFig.1. Error-State Kalman Filter 1 ![This is an image](Slide13.png) Fig.2. Error-State Kalman Filter 2 Below are results of the ESKF for roll, pitch, and yaw angles. The red line represents the estimation values, and the green is the ground truth.\nFig.3. Roll Fig.4. Pitch Fig.5. Yaw","date":1456761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456761600,"objectID":"89727553da390edd956d296c7cbc5e4d","permalink":"/post/eskf/","publishdate":"2016-03-01T00:00:00+08:00","relpermalink":"/post/eskf/","section":"post","summary":"Error state Kalman filter for attitude estimation with IMU and Magnetometer data","tags":["UAV","Robotics","Sensor Fusion","IMU"],"title":"Error State Kalman Filter","type":"post"},{"authors":null,"categories":null,"content":"Along with 3 other teammates from the State Key Laboratory of Robotics, I participated in the Unmanned Ground Systems Challenge in October 2016 and worked specifically on the environment map building and localization under GPS signal lost situation.\nThis challenge was held in a real field environment, including wild battlefield task execution, city battlefield search and investigation, and highland transportation.\nFig. 1. Our UGV platform Environment Map Built an environment map for non-structured fields with the following steps:\nFig. 2. Environment Map BUilding Procedure The laser data was collected from different lasers: single-line, 32-line, and 64-line. The data was fused after calibration and having manually filled certain points.\nFig. 3. Fuse Laser Data A road plane was fitted with a RANSAC method. Fig. 4. Point Clouds Segmentation The grid map was built with the help of the grid_map package. Finally, the road target was extracted through road skeleton extraction from the image generated from the environment map.\nFig. 5. Road Target Extraction Thus, we obtained all the perception information needed to drive a car intelligently in a field environment.\n  **Obstacle Avoidance Demo** Localization without GPS Two methods were conducted to meet the requirements.\n1. Visual Inertial Odometry I did this by combining the orb slam with IMU. Fig. 6. Visual Inertial Odometry   2. Laser Odometry   ","date":1456761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456761600,"objectID":"af76d8fc803176f391683ec4f725c4e5","permalink":"/project/unmanned-ground-systems-challenge/","publishdate":"2016-03-01T00:00:00+08:00","relpermalink":"/project/unmanned-ground-systems-challenge/","section":"project","summary":"Along with 3 other teammates from the State Key Laboratory of Robotics, I participated in the Unmanned Ground Systems Challenge in October 2016 and worked specifically on the environment map building and localization under GPS signal lost situation.","tags":["UGV","Robotics","Autonomous Robot","Computer Vision","Localization","ROS"],"title":"Unmanned Ground Systems Challenge","type":"project"},{"authors":null,"categories":null,"content":"Along with 4 other teammates from the State Key Laboratory of Robotics, I participated in the 3rd International Unmanned Aerial Vehicle Grand Prix, held at the end of October 2015. The task of this competition was to accomplish the autonomous cargo transportation task with a UAV. There were two moving platforms: both had four markers and one had 4 buckets placed on it. The UAV had to transport buckets between the two moving platforms, as shown on Figure 1.\nFig. 1. Unmanned Aerial Vehicle Grand Prix I worked specifically on the visual guidance design. Our team finished 2nd out of 20 teams in total. The vision-based guidance work can be divided into 2 parts: ellipse detection and position estimation.\nEllipse detection The ellipse detection algorithm is divided into two parts: whole ellipse detection and partial ellipse detection.\n  Whole ellipse detection\nOnce we have a new image, we need to get all the contours in that image through edge detection. Then, we calculate the AMIs (Affine Moment Invariants) to see if the contours form an ellipse. As all circles and ellipse have the same AMIs theoretical value, the contour satisfying the theoretical value can be considered as an ellipse.\n  Partial ellipse detection\nIt is common that only parts of the ellipse can be seen by a digital camera due to the limited field of view, and the whole ellipse detection method cannot detect this sort of ellipse. We then used a robust method to detect partial ellipses in the image. First, we computed the convex hull for each contour that is not classified as a whole ellipse, then fitted ellipses for each convex hull, while computing the algebraic error between the convex hulls and the fitted ellipses.\n  Position estimation Position estimation helped us get the relative position between the circle markers and the UAV, that we leveraged to track the circle to navigate the UAV. It involved three coordinate frames: world, camera and image. The principle is that we have some known coordinate point in the world frame and its corresponding coordinate point in the image frame. We then use the DLT to get the transformation between the two coordinate frames and thus obtain the position estimation.\nHere, we make the word frame’s center as the circle center, with the Z-axis of the word frame orthogonal to the circle plane. Thus, the four points on the world frame are evenly distributed on the circle as the diameter of the circle is known. Ellipse detection then yields four corresponding points in the image: we can thus estimate the rotation and transformation matrices between the circle markers and the UAV, which we use to control the UAV.\nFig. 2. Detection Results   Video 1. Indoor testing   Video 2. Competition day","date":1443628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443628800,"objectID":"bb962c09c99118478957bf3bdf2c4f15","permalink":"/project/unmanned-aerial-vehicle-grand-prix/","publishdate":"2015-10-01T00:00:00+08:00","relpermalink":"/project/unmanned-aerial-vehicle-grand-prix/","section":"project","summary":"The 3rd International Unmanned Aerial Vehicle Grand Prix with the task of autonomous cargo transportation with UAV in a GPS-Denied environment.","tags":["UAV","Robotics","Autonomous Robot","Computer Vision","Visual Guidance","Pixhawk"],"title":"The International Unmanned Aerial Vehicle Grand Prix","type":"project"},{"authors":null,"categories":null,"content":"This is a UAV ground station software designed for agriculture irrigation with functions such as information monitoring, UAV parameter setup, sensor calibration, and specific irrigation parameter setup.\n  ","date":1417363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417363200,"objectID":"9a5cf1811c1b014fc44867af4bea88a5","permalink":"/post/uav-gcs/","publishdate":"2014-12-01T00:00:00+08:00","relpermalink":"/post/uav-gcs/","section":"post","summary":"A UAV ground station software designed for agriculture irrigation.","tags":["UAV","QT","GCS"],"title":"UAV GCS for Agriculture Irrigation","type":"post"}]
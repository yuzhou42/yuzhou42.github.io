[{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536422400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536422400,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00+08:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":["Yu Zhou","Geng Qin","Feng Lin"],"categories":[],"content":"","date":1540215854,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540215854,"objectID":"291286474fc7f659d1b3b412469989df","permalink":"/publication/1/1/","publishdate":"2018-10-22T21:44:14+08:00","relpermalink":"/publication/1/1/","section":"publication","summary":"In order to develop a nano UAV in a GPS-denied environment, a visual-inertial system (VINS) consisting of a monocular camera and a low-cost inertial measurement unit (IMU) should be integrated, as they make a minimum, ultra lightweight sensor suite that enables both autonomous flight and sufficient environment awareness. Although there are numerous potential applications, getting to understand the structure of the flight platform can be cumbersome and confusing. Packages and resources provided on the Qualcomm developer network (QCN) and their official GitHub group ATLFlight contain un-unified and disorderly chunks of information. In this paper, we present an organized documentation that presents in detail, the platform structure as a whole, from hardware to software architecture, including installation and API related applications.","tags":[],"title":"Development of Nano UAV Platform for Navigation in Gps Denied Environment Using Snapdragon","type":"publication"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536422400,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00+08:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["Mingjie Lao","Xudong Chen","Feng Lin","Geng Qin","Wenqi Liu","Yu Zhou"],"categories":[],"content":"","date":1529675054,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529675054,"objectID":"eca0a75954cab1f042b923fbe8eaa92c","permalink":"/publication/3/1/","publishdate":"2018-06-22T21:44:14+08:00","relpermalink":"/publication/3/1/","section":"publication","summary":"This paper presents a visual detection and tracking framework which estimates a smooth target position for various applications on micro aerial vehicles (MAVs). The proposed framework consists of two major components: a deep learning-based detector and a correlation ﬁlter-based tracker.The detector running at a low frequency ﬁrst detects a targetand initializes the tracker. The estimated target position from the tracker will be updated by the detector when the detectionconﬁdence is high or the tracking is considered fail. Due to the limited computational power on most MAV platforms, algorithms are implemented at two separated processing units.The detector runs at a ground control station (GCS) equipped with NVIDIA GTX 1060 while the tracker runs at a MAVonboard low-cost CPU. The transmission of image and target pose information is bridged via a high-speed Wi-Fi network to minimize the latency. In our experiment, the proposed framework is able to realize real-time detection and tracking with 30 frames per second (FPS) on our system.","tags":[],"title":"Visual Target Detection and Tracking Framework Using Deep Convolutional Neural Networks for Micro Aerial Vehicles","type":"publication"},{"authors":["Zhenxing Sun","Yu Zhou","Xinghua Zhang","Haoyong Yu"],"categories":[],"content":"","date":1527860654,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527860654,"objectID":"2ecf5c00d7157af2d2aba4f77ad2d867","permalink":"/publication/2/1/","publishdate":"2018-06-01T21:44:14+08:00","relpermalink":"/publication/2/1/","section":"publication","summary":"In this study, a novel decentralized exact tracking control approach for 2-DOF planar robot manipulators with disturbances and uncertainties has been proposed. The main idea of the proposed method is to partition the whole robot manipulator into single joint and lumped disturbance. A well designed extended high gain observer (EHGO) is adopted to observe the lumped disturbance of each joint, which includes coupling, load disturbance and uncertainty, etc. Rather than coping with the robot manipulator as a whole, the proposed method is locally designed to compensate the estimated coupling and stabilize the internal states of a single joint. By doing that, each joint can be virtually decoupled from the whole robot manipulator and functions in a relatively independent way. When joints are interconnected, the stabilization of the whole robot manipulator would be obtained by only guaranteeing the localized stability of individual joint. The theoretical analyses are rigorously conducted by utilizing Lyapunov stability theorem. Simulation results illustrate the effectiveness of the proposed control scheme.","tags":[],"title":"Decentralized robust exact tracking control for 2-DOF planar robot manipulator","type":"publication"},{"authors":null,"categories":null,"content":"From March to June 2018, I worked on the proof-of-concept phase of the “Dog-Drone” project.\nThe objective is to immobilize suspicious individuals in an indoor environment by flying multiple drones aggressively around them. My work includes the following:\n Human detection and tracking Trajectory generation System integration  This work has resulted in papers published in the IEEE ICCA and IECON 2018.\n   Video 1. Dog-Drone demo\nAt this stage, to prove the concept, we are using the off-the-shelf flight platform with a visual odometry integration. Right now we have built our drone platform CT-Line due to the limitation of the commercial product and have conducted tests of flying multiple drones aggressively in an indoor environment.\n","date":1525104000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525104000,"objectID":"e6a058fba656b6194a33bf929e9b66f4","permalink":"/project/dog-drone/","publishdate":"2018-05-01T00:00:00+08:00","relpermalink":"/project/dog-drone/","section":"project","summary":"To immobilize suspicious individuals in an indoor environment by flying multiple drones aggressively around them.","tags":["UAV","Robotics","Autonomous Robot","Computer Vision","Deep Learning","Localization","Human Tracking","ROS"],"title":"Dog-Drone","type":"project"},{"authors":null,"categories":null,"content":"From March 2017 to December 2018, I worked on “Project Micrathene” with a Ph.D. student who focused on controls. The objective of this project was to develop an autonomous launch, tracking and landing system for an Unmanned Aerial Vehicle (UAV). The object to track and land on was a moving ground platform, and everything was to be done without GPS.\nIn this project, vision is used to estimate the pose, the translation and rotation relationship, between the UGV and the UAV. The pose was used for flight control during different states of the UAV.\nThe hardware platform contains:\n DJI Matrice 100 fly platform Point Grey Chameleon 3 camera NUC onboard computer. Omnidirectional mobility platform  Communication between the onboard and offboard computer is established over Wi-Fi.\nThe software involves:\n Ubuntu ROS Kinetic OpenCV DJI SDK  Fig. 1. Hardware and Software Platform\nA marker was designed to obtain the relative pose between the UAV and the UGV platform. There are four coordinate systems: image, camera, marker and UAV. The transformation matrix between different coordinate systems is shown on Figure 2. n order to control the UAV, the pose between the camera and the marker, which is [R2 t2], has to be calculated with the PnP method. The pose between the UAV and the moving platform can then be obtained through another coordinate transform.\nFig. 2. Transformation matrix\nThe software design of this project is shown on Figure 3:\n Obtain undistorted images through image processing;\n Send these images to the marker detection process to get the camera pose w.r.t. the marker;\n In the main process, a state machine was run to process all the data and commands, including the pose between the moving platform and the UAV, sensor data from DJI SDK and the command from the off-board computer.\n  Fig. 3. Software design\nThe final demonstration is shown in the following video:   Demo Video\n","date":1489507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489507200,"objectID":"28c2aaddac3aee58eddbc2eb08fefce2","permalink":"/project/gps-denied-vision-control-of-uav/","publishdate":"2017-03-15T00:00:00+08:00","relpermalink":"/project/gps-denied-vision-control-of-uav/","section":"project","summary":"Develop an autonomous launch, tracking and landing system for an Unmanned Aerial Vehicle (UAV).","tags":["UAV","Robotics","Autonomous Robot","Computer Vision","ROS","Matrice 100","DJI SDK"],"title":"GPS-Denied Vision Control of UAV","type":"project"},{"authors":[],"categories":null,"content":"Click on the Slides button above to view the built-in slides feature.\n Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483200000,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00+08:00","relpermalink":"/talk/example/","section":"talk","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"This project was to calibrate the translation and rotation matrices between camera and laser coordinate systems to provide laser points with color information from the image for further applications such as segmentation.\nThe QR code board was used as a marker, which could be detected in both point clouds and image. I matched the center point pair of the QR code from the 2D points in the image and that of the 3D points in laser coordinates, and used the PnP method to get the relation between the two coordinates. The calibration procedure and fusion results are shown on Figure 1.\nFig. 1. Calibration Procedure\n   Video 1. Fusion results\n   Video 2. Fusion results\n","date":1483200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483200000,"objectID":"c22d14f5f82c1ffa7fb25808f89eb443","permalink":"/project/laser-camera-calibration/","publishdate":"2017-01-01T00:00:00+08:00","relpermalink":"/project/laser-camera-calibration/","section":"project","summary":"This project was to calibrate the translation and rotation matrices between camera and laser coordinate systems to provide laser points with color information from the image for further applications such as segmentation.","tags":["Laser","Sensor Fusion","Computer Vision","ROS"],"title":"Laser Camera Fusion","type":"project"},{"authors":["Liying Yang","Bin Xiao","Yu Zhou","Yuqing He","Hongzhi Zhang","Jianda Han"],"categories":[],"content":"","date":1466603054,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466603054,"objectID":"c5e245732c833b61271b14304335aed6","permalink":"/publication/4/1/","publishdate":"2016-06-22T21:44:14+08:00","relpermalink":"/publication/4/1/","section":"publication","summary":"In 2015 International UAV Innovation Grand Prix the competition, the cargo transport task is assumed as: there are 4 buckets placed in four circles on one moving platform. Firstly, the unmanned aerial vehicle (UAV) is required to identify circle targets and the black and white id marker near the circle on one moving platform, then the UAV chosen a target bucket, tracked and transported it to the other moving platform, until all 4 buckets are transported from one moving platform to the other. In order to accomplish the cargo transport task, a method of a real-time vision-based GPS-denied multiple object tracking for UAV is developed. The Pixhawk controller is used to achieve tracking, that the relative distance and velocity between the target and UAV is estimated by the image. Finally, the experimental results proved the effectiveness and robustness of the algorithm.","tags":[],"title":"A robust real-time vision based GPS-denied navigation system of UAV","type":"publication"},{"authors":[],"categories":null,"content":" Academic makes it easy to create a beautiful website for free using Markdown. Customize anything on your site with widgets, themes, and language packs.\nFollow our easy step by step guide to learn how to build your own free website with Academic. Check out the personal demo or the business demo of what you\u0026rsquo;ll get in less than 10 minutes.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt   \nKey features:\n Easily manage various content including homepage, blog posts, publications, talks, and projects Extensible via color themes and widgets/plugins Write in Markdown for easy formatting and code highlighting, with LaTeX for mathematical expressions Social/academic network linking, Google Analytics, and Disqus comments Responsive and mobile friendly Simple and refreshing one page design Multilingual and easy to customize  Color Themes Academic is available in different color themes and font themes.\n         Ecosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Quick install using your web browser  Install Academic with Netlify  Netlify will provide you with a customizable URL to access your new site  On GitHub, go to your newly created academic-kickstart repository and edit config.toml to personalize your site. Shortly after saving the file, your site will automatically update Read the Quick Start Guide to learn how to add Markdown content. For inspiration, refer to the Markdown content which powers the Demo  Install with Git Prerequisites:\n Download and install Git Download and install Hugo   Fork the Academic Kickstart repository and clone your fork with Git:\ngit clone https://github.com/sourcethemes/academic-kickstart.git My_Website  Note that if you forked Academic Kickstart, the above command should be edited to clone your fork, i.e. replace sourcethemes with your GitHub username.\n Initialize the theme:\ncd My_Website git submodule update --init --recursive   Install with ZIP  Download and extract Academic Kickstart Download and extract the Academic theme to the themes/academic/ folder from the above step  Install with RStudio View the guide to installing Academic with RStudio\nQuick start  If you installed on your computer, view your new website by running the following command:\nhugo server  Now visit localhost:1313 and your new Academic powered website will appear. Otherwise, if using Netlify, they will provide you with your URL.\n Read the Quick Start Guide to learn how to add Markdown content, customize your site, and deploy it. For inspiration, refer to the Markdown content which powers the Demo\n Build your site by running the hugo command. Then host it for free using Github Pages or Netlify (refer to the first installation method). Alternatively, copy the generated public/ directory (by FTP, Rsync, etc.) to your production web server (such as a university\u0026rsquo;s hosting service).\n  Updating Feel free to star the project on Github to help keep track of updates and check out the release notes prior to updating your site.\nBefore updating the framework, it is recommended to make a backup of your entire website directory (or at least your themes/academic directory) and record your current version number.\nBy default, Academic is installed as a Git submodule which can be updated by running the following command:\ngit submodule update --remote --merge  Check out the update guide for full instructions and alternative methods.\nFeedback \u0026amp; Contributing Please use the issue tracker to let me know about any bugs or feature requests, or alternatively make a pull request.\nFor support, head over to the Hugo discussion forum.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515772800,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2016-04-20T00:00:00+08:00","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website or blog in under 10 minutes.","tags":["Academic"],"title":"Academic: the website designer for Hugo","type":"post"},{"authors":null,"categories":null,"content":"The error-state Kalman filter (ESKF) is one of the tools we may use for combining IMU with magnetometer data to obtain a robust attitude estimation. It has many benefits such as avoiding issues related to over-parameterization and the consequent risk of the singularity of the involved covariance matrices. The formulation of the ESKF algorithm used for attitude estimation is as follows:\nFig.1. Error-State Kalman Filter 1 Fig.2. Error-State Kalman Filter 2\nBelow are results of the ESKF for roll, pitch, and yaw angles. The red line represents the estimation values, and the green is the ground truth.\nFig.3. Roll Fig.4. Pitch Fig.5. Yaw\n","date":1456761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456761600,"objectID":"a125b209bbcdc387cf87dd6edb92913a","permalink":"/project/eskf/","publishdate":"2016-03-01T00:00:00+08:00","relpermalink":"/project/eskf/","section":"project","summary":"Error state Kalman filter for attitude estimation with IMU and Magnetometer data","tags":["UAV","Robotics","Sensor Fusion","IMU"],"title":"Error State Kalman Filter","type":"project"},{"authors":null,"categories":null,"content":" Along with 3 other teammates from the State Key Laboratory of Robotics, I participated in the Unmanned Ground Systems Challenge in October 2016 and worked specifically on the environment map building and localization under GPS signal lost situation.\nThis challenge was held in a real field environment, including wild battlefield task execution, city battlefield search and investigation, and highland transportation.\nFig. 1. Our UGV platform\nEnvironment Map Built an environment map for non-structured fields with the following steps:\nFig. 2. Environment Map BUilding Procedure\nThe laser data was collected from different lasers: single-line, 32-line, and 64-line. The data was fused after calibration and having manually filled certain points.\nFig. 3. Fuse Laser Data\nA road plane was fitted with a RANSAC method. Fig. 4. Point Clouds Segmentation\nThe grid map was built with the help of the grid_map package. Finally, the road target was extracted through road skeleton extraction from the image generated from the environment map.\nFig. 5. Road Target Extraction\nThus, we obtained all the perception information needed to drive a car intelligently in a field environment.\n   Obstacle Avoidance Demo\nLocalization without GPS Two methods were conducted to meet the requirements.\n1. Visual Inertial Odometry I did this by combining the orb slam with IMU. Fig. 6. Visual Inertial Odometry\n  2. Laser Odometry   ","date":1456761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456761600,"objectID":"af76d8fc803176f391683ec4f725c4e5","permalink":"/project/unmanned-ground-systems-challenge/","publishdate":"2016-03-01T00:00:00+08:00","relpermalink":"/project/unmanned-ground-systems-challenge/","section":"project","summary":"Along with 3 other teammates from the State Key Laboratory of Robotics, I participated in the Unmanned Ground Systems Challenge in October 2016 and worked specifically on the environment map building and localization under GPS signal lost situation.\nThis challenge was held in a real field environment, including wild battlefield task execution, city battlefield search and investigation, and highland transportation.\nFig. 1. Our UGV platform\nEnvironment Map Built an environment map for non-structured fields with the following steps:","tags":["UGV","Robotics","Autonomous Robot","Computer Vision","Localization","ROS"],"title":"Unmanned Ground Systems Challenge","type":"project"},{"authors":null,"categories":null,"content":" Along with 4 other teammates from the State Key Laboratory of Robotics, I participated in the 3rd International Unmanned Aerial Vehicle Grand Prix, held at the end of October 2015. The task of this competition was to accomplish the autonomous cargo transportation task with a UAV. There were two moving platforms: both had four markers and one had 4 buckets placed on it. The UAV had to transport buckets between the two moving platforms, as shown on Figure 1.\nFig. 1. Unmanned Aerial Vehicle Grand Prix\nI worked specifically on the visual guidance design. Our team finished 2nd out of 20 teams in total. The vision-based guidance work can be divided into 2 parts: ellipse detection and position estimation.\nEllipse detection The ellipse detection algorithm is divided into two parts: whole ellipse detection and partial ellipse detection.\n Whole ellipse detection\nOnce we have a new image, we need to get all the contours in that image through edge detection. Then, we calculate the AMIs (Affine Moment Invariants) to see if the contours form an ellipse. As all circles and ellipse have the same AMIs theoretical value, the contour satisfying the theoretical value can be considered as an ellipse.\n Partial ellipse detection\nIt is common that only parts of the ellipse can be seen by a digital camera due to the limited field of view, and the whole ellipse detection method cannot detect this sort of ellipse. We then used a robust method to detect partial ellipses in the image. First, we computed the convex hull for each contour that is not classified as a whole ellipse, then fitted ellipses for each convex hull, while computing the algebraic error between the convex hulls and the fitted ellipses.\n  Position estimation Position estimation helped us get the relative position between the circle markers and the UAV, that we leveraged to track the circle to navigate the UAV. It involved three coordinate frames: world, camera and image. The principle is that we have some known coordinate point in the world frame and its corresponding coordinate point in the image frame. We then use the DLT to get the transformation between the two coordinate frames and thus obtain the position estimation.\nHere, we make the word frame’s center as the circle center, with the Z-axis of the word frame orthogonal to the circle plane. Thus, the four points on the world frame are evenly distributed on the circle as the diameter of the circle is known. Ellipse detection then yields four corresponding points in the image: we can thus estimate the rotation and transformation matrices between the circle markers and the UAV, which we use to control the UAV.\nFig. 2. Detection Results\n   Video 1. Indoor testing\n   Video 2. Competition day\n","date":1443628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443628800,"objectID":"bb962c09c99118478957bf3bdf2c4f15","permalink":"/project/unmanned-aerial-vehicle-grand-prix/","publishdate":"2015-10-01T00:00:00+08:00","relpermalink":"/project/unmanned-aerial-vehicle-grand-prix/","section":"project","summary":"The 3rd International Unmanned Aerial Vehicle Grand Prix with the task of autonomous cargo transportation with UAV in a GPS-Denied environment.","tags":["UAV","Robotics","Autonomous Robot","Computer Vision","Visual Guidance","Pixhawk"],"title":"The International Unmanned Aerial Vehicle Grand Prix","type":"project"},{"authors":null,"categories":null,"content":"This is a UAV ground station software designed for agriculture irrigation with functions such as information monitoring, UAV parameter setup, sensor calibration, and specific irrigation parameter setup.\n  ","date":1417363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417363200,"objectID":"c9412d43d753d1847b25931d43af97b9","permalink":"/project/uav-gcs/","publishdate":"2014-12-01T00:00:00+08:00","relpermalink":"/project/uav-gcs/","section":"project","summary":"A UAV ground station software designed for agriculture irrigation.","tags":["UAV","QT","GCS"],"title":"UAV GCS for Agriculture Irrigation","type":"project"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]
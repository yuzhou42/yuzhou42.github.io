<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>UAV | Yu Zhou</title>
    <link>/tag/uav/</link>
      <atom:link href="/tag/uav/index.xml" rel="self" type="application/rss+xml" />
    <description>UAV</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Yu Zhou</copyright><lastBuildDate>Mon, 01 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>UAV</title>
      <link>/tag/uav/</link>
    </image>
    
    <item>
      <title>Toward Autonomy of Micro Aerial Vehicles in Unknown Environments</title>
      <link>/project/toward-autonomy-of-micro-aerial-vehicles-in-unknown-environments/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/project/toward-autonomy-of-micro-aerial-vehicles-in-unknown-environments/</guid>
      <description>&lt;p&gt;Since January 2019, I have been working on developing a  MAV system that can navigate autonomously in GPS-denied and obstacle-cluttered environments. Various advanced technologies have been designed, including a stereo visual-inertial state estimation that can handle scenarios without external localization resource, a GPU-based EDF mapping that significantly improves the real-time performance without sacrificing accuracy as well as a model predictive local motion planning with BSCPs solved by PSO, providing increased performance for tasks that require precise and timely maneuver. The results of simulation and real flight testing in both indoor and semi-open scenarios with various tasks have proved the effectiveness of the proposed system for challenging practical applications.&lt;/p&gt;
&lt;p&gt;This work has resulted in the paper “Towards Autonomy of Micro Aerial Vehicles in Unknown and Global Positioning System Denied Environments” published in the IEEE Transactions on Industrial Electronics.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/KUKzsnORm-4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;center&gt;Demo&lt;/center&gt;
</description>
    </item>
    
    <item>
      <title>Robot web-based user interface</title>
      <link>/post/robot_web_ui/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/robot_web_ui/</guid>
      <description>&lt;h3 id=&#34;overview&#34;&gt;overview&lt;/h3&gt;
&lt;p&gt;This was a web-based UI (as shown in the image),  I developed with the purpose of connecting to multiple drones via their  IP address and  reporting back the status of each drone, e.g. position, video feed and health status. The user may send commands (takeoff, mission, swarm, hover, landing) to any connected drone directly from the UI. Google map was inserted to show real time locations and trajectories.&lt;/p&gt;
&lt;h3 id=&#34;tutorials-and-useful-websites&#34;&gt;Tutorials and useful websites&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://medium.com/husarion-blog/bootstrap-4-ros-creating-a-web-ui-for-your-robot-9a77a8e373f9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bootstrap 4 + ROS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.w3schools.com/bootstrap4/default.asp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bootstrap 4 Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://robotwebtools.org/tools.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ROBOTWEBTOOLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://bootswatch.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Theme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.rapidtables.com/web/color/html-color-codes.htmls&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML color code&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Dog-Drone</title>
      <link>/project/dog-drone/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>/project/dog-drone/</guid>
      <description>&lt;p&gt;From March to June 2018, I worked on the proof-of-concept phase of the “Dog-Drone” project.&lt;br&gt;
The objective is to immobilize suspicious individuals in an indoor environment by flying multiple drones aggressively around them.
My work includes the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Human detection and tracking&lt;/li&gt;
&lt;li&gt;Trajectory generation&lt;/li&gt;
&lt;li&gt;System integration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This work has resulted in papers published in the IEEE ICCA and IECON 2018.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/kSz1FB7mgqU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;center&gt;Dog-Drone demo&lt;/center&gt;
&lt;p&gt;At this stage, to prove the concept, we are using the off-the-shelf flight platform with a visual odometry integration.
Right now we have built our drone platform CT-Line due to the limitation of the commercial product and
have conducted tests of flying multiple drones aggressively in an indoor environment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GPS-Denied Vision Control of UAV</title>
      <link>/project/gps-denied-vision-control-of-uav/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      <guid>/project/gps-denied-vision-control-of-uav/</guid>
      <description>&lt;p&gt;From March 2017 to December 2018, I worked on “Project Micrathene” with a Ph.D. student who focused on controls.
The objective of this project was to develop an autonomous launch, tracking and landing system for an Unmanned Aerial Vehicle (UAV).
The object to track and land on was a moving ground platform, and everything was to be done without GPS.&lt;/p&gt;
&lt;p&gt;In this project, vision is used to estimate the pose,
the translation and rotation relationship, between the UGV and the UAV.
The pose was used for flight control during different states of the UAV.&lt;/p&gt;
&lt;p&gt;The hardware platform contains:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DJI Matrice 100 fly platform&lt;/li&gt;
&lt;li&gt;Point Grey Chameleon 3 camera&lt;/li&gt;
&lt;li&gt;NUC onboard computer.&lt;/li&gt;
&lt;li&gt;Omnidirectional mobility platform&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Communication between the onboard and offboard computer is established over Wi-Fi.&lt;/p&gt;
&lt;p&gt;The software involves:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ubuntu&lt;/li&gt;
&lt;li&gt;ROS Kinetic&lt;/li&gt;
&lt;li&gt;OpenCV&lt;/li&gt;
&lt;li&gt;DJI SDK&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Slide2.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 1. Hardware and Software Platform&lt;/center&gt;
&lt;p&gt;A marker was designed to obtain the relative pose between the UAV and the UGV platform.
There are four coordinate systems: image, camera, marker and UAV.
The transformation matrix between different coordinate systems is shown on Figure 2.
n order to control the UAV, the pose between the camera and the marker, which is [R2 t2], has to be calculated with the PnP method.
The pose between the UAV and the moving platform can then be obtained through another coordinate transform.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Slide3.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 2. Transformation matrix&lt;/center&gt;
&lt;p&gt;The software design of this project is shown on Figure 3:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Obtain undistorted images through image processing;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Send these images to the marker detection process to get the camera pose w.r.t. the marker;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the main process, a state machine was run to process all the data and commands,
including the pose between the moving platform and the UAV, sensor data from DJI SDK and the command from the off-board computer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;Slide4.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 3. Software design&lt;/center&gt;
&lt;p&gt;The final demonstration is shown in the following video:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/cIJGd6TkZRM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;center&gt;Demo Video&lt;/center&gt;</description>
    </item>
    
    <item>
      <title>Error State Kalman Filter</title>
      <link>/post/eskf/</link>
      <pubDate>Tue, 01 Mar 2016 00:00:00 +0000</pubDate>
      <guid>/post/eskf/</guid>
      <description>&lt;p&gt;The error-state Kalman filter (ESKF) is one of the tools we may use for
combining IMU with magnetometer data to obtain a robust attitude estimation.
It has many benefits such as avoiding issues related to over-parameterization and the consequent risk of the singularity
of the involved covariance matrices.
The formulation of the ESKF algorithm used for attitude estimation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Slide12.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig.1. Error-State Kalman Filter 1&lt;/center&gt;
![This is an image](Slide13.png)
&lt;center&gt;Fig.2. Error-State Kalman Filter 2&lt;/center&gt;
&lt;p&gt;Below are results of the ESKF for roll, pitch, and yaw angles.
The red line represents the estimation values, and the green is the ground truth.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig.3. Roll&lt;/center&gt;
&lt;p&gt;&lt;img src=&#34;eskf_2.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig.4. Pitch&lt;/center&gt;
&lt;p&gt;&lt;img src=&#34;eskf_3.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig.5. Yaw&lt;/center&gt;</description>
    </item>
    
    <item>
      <title>The International Unmanned Aerial Vehicle Grand Prix</title>
      <link>/project/unmanned-aerial-vehicle-grand-prix/</link>
      <pubDate>Thu, 01 Oct 2015 00:00:00 +0000</pubDate>
      <guid>/project/unmanned-aerial-vehicle-grand-prix/</guid>
      <description>&lt;p&gt;Along with 4 other teammates from the 
&lt;a href=&#34;http://english.sia.cas.cn/rh/rp/201408/t20140814_125856.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;State Key Laboratory of Robotics&lt;/a&gt;,
I participated in the 3rd International Unmanned Aerial Vehicle Grand Prix,
held at the end of October 2015.
The task of this competition was to accomplish the autonomous cargo transportation task with a UAV.
There were two moving platforms:
both had four markers and one had 4 buckets placed on it.
The UAV had to transport buckets between the two moving platforms, as shown on Figure 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Slide15.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 1. Unmanned Aerial Vehicle Grand Prix&lt;/center&gt;
&lt;p&gt;I worked specifically on the visual guidance design.
Our team finished 2nd out of 20 teams in total.
The vision-based guidance work can be divided into 2 parts:
ellipse detection and position estimation.&lt;/p&gt;
&lt;h4 id=&#34;ellipse-detection&#34;&gt;Ellipse detection&lt;/h4&gt;
&lt;p&gt;The ellipse detection algorithm is divided into two parts: whole ellipse detection and partial ellipse detection.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Whole ellipse detection&lt;/p&gt;
&lt;p&gt;Once we have a new image, we need to get all the contours in that image through edge detection.
Then, we calculate the AMIs (Affine Moment Invariants) to see if the contours form an ellipse.
As all circles and ellipse have the same AMIs theoretical value,
the contour satisfying the theoretical value can be considered as an ellipse.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Partial ellipse detection&lt;/p&gt;
&lt;p&gt;It is common that only parts of the ellipse can be seen by a digital camera due to the limited field of view,
and the whole ellipse detection method cannot detect this sort of ellipse.
We then used a robust method to detect partial ellipses in the image.
First, we computed the convex hull for each contour that is not classified as a whole ellipse,
then fitted ellipses for each convex hull,
while computing the algebraic error between the convex hulls and the fitted ellipses.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;position-estimation&#34;&gt;Position estimation&lt;/h4&gt;
&lt;p&gt;Position estimation helped us get the relative position between the circle markers and the UAV,
that we leveraged to track the circle to navigate the UAV.
It involved three coordinate frames: world, camera and image.
The principle is that we have some known coordinate point in the world frame and its corresponding coordinate point in the image frame.
We then use the DLT to get the transformation between the two coordinate frames and thus obtain the position estimation.&lt;/p&gt;
&lt;p&gt;Here, we make the word frame’s center as the circle center,
with the Z-axis of the word frame orthogonal to the circle plane.
Thus, the four points on the world frame are evenly distributed on the circle as the diameter of the circle is known.
Ellipse detection then yields four corresponding points in the image:
we can thus estimate the rotation and transformation matrices between the circle markers and the UAV, which we use to control the UAV.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Slide16.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 2. Detection Results&lt;/center&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/3368_Q3Q8q0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;center&gt;Video 1. Indoor testing&lt;/center&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/0SGRLABnbwc&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;center&gt;Video 2. Competition day&lt;/center&gt;</description>
    </item>
    
    <item>
      <title>UAV GCS for Agriculture Irrigation</title>
      <link>/post/uav-gcs/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      <guid>/post/uav-gcs/</guid>
      <description>&lt;p&gt;This is a UAV ground station software designed for agriculture irrigation with functions such as information monitoring, UAV parameter setup, sensor calibration, and specific irrigation parameter setup.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/8a9DuZseNoI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
  </channel>
</rss>

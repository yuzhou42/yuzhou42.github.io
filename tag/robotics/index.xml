<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robotics | Yu Zhou</title>
    <link>/tag/robotics/</link>
      <atom:link href="/tag/robotics/index.xml" rel="self" type="application/rss+xml" />
    <description>Robotics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Yu Zhou</copyright><lastBuildDate>Mon, 01 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Robotics</title>
      <link>/tag/robotics/</link>
    </image>
    
    <item>
      <title>Toward Autonomy of Micro Aerial Vehicles in Unknown Environments</title>
      <link>/project/toward-autonomy-of-micro-aerial-vehicles-in-unknown-environments/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/project/toward-autonomy-of-micro-aerial-vehicles-in-unknown-environments/</guid>
      <description>&lt;p&gt;Since January 2019, I have been working on developing a  MAV system that can navigate autonomously in GPS-denied and obstacle-cluttered environments. Various advanced technologies have been designed, including a stereo visual-inertial state estimation that can handle scenarios without external localization resource, a GPU-based EDF mapping that significantly improves the real-time performance without sacrificing accuracy as well as a model predictive local motion planning with BSCPs solved by PSO, providing increased performance for tasks that require precise and timely maneuver. The results of simulation and real flight testing in both indoor and semi-open scenarios with various tasks have proved the effectiveness of the proposed system for challenging practical applications.&lt;/p&gt;
&lt;p&gt;This work has resulted in the paper “Towards Autonomy of Micro Aerial Vehicles in Unknown and Global Positioning System Denied Environments” published in the IEEE Transactions on Industrial Electronics.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/KUKzsnORm-4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;center&gt;Demo&lt;/center&gt;
</description>
    </item>
    
    <item>
      <title>Dog-Drone</title>
      <link>/project/dog-drone/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>/project/dog-drone/</guid>
      <description>&lt;body&gt;&lt;img src=&#34;hardware.png&#34;  width=&#34;600&#34; height=&#34;&#34; &gt;&lt;p style=&#34;text-align:center;&#34;&gt;System Diagram&lt;/p&gt;&lt;/body&gt;
&lt;/br&gt;
From March to June 2018, I worked on the proof-of-concept phase of the “Dog-Drone” project. The objective is to immobilize suspicious individuals in an indoor environment by flying multiple drones aggressively around them. My work includes trajectory generation and system integration.
&lt;p&gt;This work has resulted in papers published in the IEEE ICCA and IECON 2018. We provide step to step guide about building a drone with snapdragon platform and present an organized documentation that presents in detail, the platform structure as a whole, from hardware to software architecture, including installation and API related applications.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/kSz1FB7mgqU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;center&gt;Dog-Drone demo&lt;/center&gt;
&lt;/br&gt;
At previous stage, to prove the concept, we are using the off-the-shelf flight platform with a visual odometry integration. Right now we have built our new drone platform due to the limitation of the commercial product and have conducted tests of flying multiple drones aggressively in an indoor environment. 
</description>
    </item>
    
    <item>
      <title>GPS-Denied Vision Control of UAV</title>
      <link>/project/gps-denied-vision-control-of-uav/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      <guid>/project/gps-denied-vision-control-of-uav/</guid>
      <description>&lt;p&gt;From March 2017 to December 2018, I worked on “Project Micrathene” with a Ph.D. student who focused on controls. The objective of this project was to develop an autonomous launch, tracking and landing system for an Unmanned Aerial Vehicle (UAV). The object to track and land on was a moving ground platform, and everything was to be done without GPS.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/cIJGd6TkZRM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;center&gt;Demo Video&lt;/center&gt;
&lt;/br&gt;
&lt;p&gt;In this project, vision is used to estimate the pose,
the translation and rotation relationship, between the UGV and the UAV.
The pose was used for flight control during different states of the UAV.&lt;/p&gt;
&lt;p&gt;The hardware platform contains:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DJI Matrice 100 fly platform&lt;/li&gt;
&lt;li&gt;Point Grey Chameleon 3 camera&lt;/li&gt;
&lt;li&gt;NUC onboard computer.&lt;/li&gt;
&lt;li&gt;Omnidirectional mobility platform&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Slide2.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 1. Hardware and Software Platform&lt;/center&gt;
&lt;/br&gt;
Communication between the onboard and offboard computer is established over Wi-Fi. The software involves: Ubuntu, ROS Kinetic, OpenCV and DJI SDK.
&lt;p&gt;A marker was designed to obtain the relative pose between the UAV and the UGV platform.
There are four coordinate systems: image, camera, marker and UAV.
The transformation matrix between different coordinate systems is shown on Figure 2.
In order to control the UAV, the pose between the camera and the marker, which is $[R2\quad t2]$, has to be calculated with the PnP method.
The pose between the UAV and the moving platform can then be obtained through another coordinate transform.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Slide3.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 2. Transformation matrix&lt;/center&gt;
&lt;p&gt;The software design of this project is shown on Figure 3:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Obtain undistorted images through image processing;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Send these images to the marker detection process to get the camera pose w.r.t. the marker;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the main process, a state machine was run to process all the data and commands,
including the pose between the moving platform and the UAV, sensor data from DJI SDK and the command from the off-board computer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;Slide4.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 3. Software design&lt;/center&gt;
</description>
    </item>
    
    <item>
      <title>Error State Kalman Filter</title>
      <link>/post/eskf/</link>
      <pubDate>Tue, 01 Mar 2016 00:00:00 +0000</pubDate>
      <guid>/post/eskf/</guid>
      <description>&lt;p&gt;The error-state Kalman filter (ESKF) is one of the tools we may use for
combining IMU with magnetometer data to obtain a robust attitude estimation.
It has many benefits such as avoiding issues related to over-parameterization and the consequent risk of the singularity
of the involved covariance matrices.
The formulation of the ESKF algorithm used for attitude estimation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Slide12.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig.1. Error-State Kalman Filter 1&lt;/center&gt;
&lt;/br&gt;
&lt;p&gt;&lt;img src=&#34;Slide13.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig.2. Error-State Kalman Filter 2&lt;/center&gt;
&lt;/br&gt;
&lt;p&gt;Below are results of the ESKF for roll, pitch, and yaw angles.
The red line represents the estimation values, and the green is the ground truth.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig.3. Roll&lt;/center&gt;
&lt;/br&gt;
&lt;p&gt;&lt;img src=&#34;eskf_2.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig.4. Pitch&lt;/center&gt;
&lt;/br&gt;
&lt;p&gt;&lt;img src=&#34;eskf_3.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig.5. Yaw&lt;/center&gt;</description>
    </item>
    
    <item>
      <title>Unmanned Ground Systems Challenge</title>
      <link>/project/unmanned-ground-systems-challenge/</link>
      <pubDate>Tue, 01 Mar 2016 00:00:00 +0000</pubDate>
      <guid>/project/unmanned-ground-systems-challenge/</guid>
      <description>&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;p&gt;Along with 3 other teammates from the 
&lt;a href=&#34;http://english.sia.cas.cn/rh/rp/201408/t20140814_125856.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;State Key Laboratory of Robotics&lt;/a&gt;,
I participated in the Unmanned Ground Systems Challenge in October 2016 and worked specifically on the environment map building and localization under GPS signal lost situation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ugv_1.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 1. Our UGV platform&lt;/center&gt;
&lt;h3 id=&#34;environment-map&#34;&gt;Environment Map&lt;/h3&gt;
&lt;p&gt;Built an environment map for non-structured fields with the following steps:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Slide6.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 2. Environment Map BUilding Procedure&lt;/center&gt;
&lt;/br&gt;
&lt;p&gt;The laser data was collected from different lasers: single-line, 32-line, and 64-line.
The data was fused after calibration and having manually filled certain points.
&lt;img src=&#34;Slide7.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 3. Fuse Laser Data&lt;/center&gt;
&lt;/br&gt;
A road plane was fitted with a RANSAC method.
&lt;p&gt;&lt;img src=&#34;Slide8.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 4. Point Clouds Segmentation&lt;/center&gt;
&lt;/br&gt;
&lt;p&gt;The grid map was built with the help of the 
&lt;a href=&#34;https://github.com/ANYbotics/grid_map&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;grid_map&lt;/a&gt; package.
Finally, the road target was extracted through road skeleton extraction from the image generated from the environment map.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Slide9.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 5. Road Target Extraction&lt;/center&gt;
&lt;/br&gt;
Thus, we obtained all the perception information needed to drive a car intelligently in a field environment.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/AUbdU3WGoK8&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;center&gt;Obstacle Avoidance Demo&lt;/center&gt;
&lt;h3 id=&#34;localization-without-gps&#34;&gt;Localization without GPS&lt;/h3&gt;
&lt;p&gt;Two methods were conducted to meet the requirements.&lt;/p&gt;
&lt;h4 id=&#34;1-visual-inertial-odometry&#34;&gt;1. Visual Inertial Odometry&lt;/h4&gt;
&lt;p&gt;I did this by combining the orb slam with IMU.
&lt;img src=&#34;Slide10.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 6. Visual Inertial Odometry&lt;/center&gt;
&lt;/br&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/RRAzWU1_SSE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;/br&gt;
&lt;h4 id=&#34;2-laser-odometry&#34;&gt;2. Laser Odometry&lt;/h4&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/c0VvjAZcUNo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>The International Unmanned Aerial Vehicle Grand Prix</title>
      <link>/project/unmanned-aerial-vehicle-grand-prix/</link>
      <pubDate>Thu, 01 Oct 2015 00:00:00 +0000</pubDate>
      <guid>/project/unmanned-aerial-vehicle-grand-prix/</guid>
      <description>&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;p&gt;Along with 4 other teammates from the 
&lt;a href=&#34;http://english.sia.cas.cn/rh/rp/201408/t20140814_125856.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;State Key Laboratory of Robotics&lt;/a&gt;,
I participated in the 3rd International Unmanned Aerial Vehicle Grand Prix,
held at the end of October 2015.
The task of this competition was to accomplish the autonomous cargo transportation task with a UAV.
There were two moving platforms:
both had four markers and one had 4 buckets placed on it.
The UAV had to transport buckets between the two moving platforms, as shown on Figure 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Slide15.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 1. Unmanned Aerial Vehicle Grand Prix&lt;/center&gt;
&lt;/br&gt;
&lt;h2 id=&#34;visual-guidance&#34;&gt;Visual guidance&lt;/h2&gt;
&lt;p&gt;I worked specifically on the visual guidance design.
Our team finished 2nd out of 20 teams in total.
The vision-based guidance work can be divided into 2 parts:
ellipse detection and position estimation.&lt;/p&gt;
&lt;h3 id=&#34;ellipse-detection&#34;&gt;Ellipse detection&lt;/h3&gt;
&lt;p&gt;The ellipse detection algorithm is divided into two parts: whole ellipse detection and partial ellipse detection.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Whole ellipse detection&lt;/p&gt;
&lt;p&gt;Once we have a new image, we need to get all the contours in that image through edge detection.
Then, we calculate the AMIs (Affine Moment Invariants) to see if the contours form an ellipse.
As all circles and ellipse have the same AMIs theoretical value,
the contour satisfying the theoretical value can be considered as an ellipse.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Partial ellipse detection&lt;/p&gt;
&lt;p&gt;It is common that only parts of the ellipse can be seen by a digital camera due to the limited field of view,
and the whole ellipse detection method cannot detect this sort of ellipse.
We then used a robust method to detect partial ellipses in the image.
First, we computed the convex hull for each contour that is not classified as a whole ellipse,
then fitted ellipses for each convex hull,
while computing the algebraic error between the convex hulls and the fitted ellipses.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;position-estimation&#34;&gt;Position estimation&lt;/h3&gt;
&lt;p&gt;Position estimation helped us get the relative position between the circle markers and the UAV,
that we leveraged to track the circle to navigate the UAV.
It involved three coordinate frames: world, camera and image.
The principle is that we have some known coordinate point in the world frame and its corresponding coordinate point in the image frame.
We then use the DLT to get the transformation between the two coordinate frames and thus obtain the position estimation.&lt;/p&gt;
&lt;p&gt;Here, we make the word frame’s center as the circle center,
with the Z-axis of the word frame orthogonal to the circle plane.
Thus, the four points on the world frame are evenly distributed on the circle as the diameter of the circle is known.
Ellipse detection then yields four corresponding points in the image:
we can thus estimate the rotation and transformation matrices between the circle markers and the UAV, which we use to control the UAV.
&lt;img src=&#34;Slide16.png&#34; alt=&#34;This is an image&#34;&gt;&lt;/p&gt;
&lt;center&gt;Fig. 2. Detection Results&lt;/center&gt;
&lt;/br&gt;
&lt;h1 id=&#34;videos&#34;&gt;Videos&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/3368_Q3Q8q0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;center&gt;Video 1. Indoor testing&lt;/center&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/0SGRLABnbwc&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;center&gt;Video 2. Competition day&lt;/center&gt;</description>
    </item>
    
  </channel>
</rss>
